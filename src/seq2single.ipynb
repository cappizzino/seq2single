{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Place Recognition under opposing viewpoints with varying scene appearance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses pre-computed features to demonstrate the approach described in the ICRA 2019 paper titled:\n",
    "\n",
    "*Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation*\n",
    "\n",
    "The example dataset used here - a subset of the [Oxford Robotcar dataset](https://robotcar-dataset.robots.ox.ac.uk/) - compares the **rear-view** images from the **Autumn** traverse to the **front-view** images from the **Summer** traverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys, cv2, time\n",
    "\n",
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-computed data paths\n",
    "\n",
    "refPath_depthData = \"../precomputed/depthData/1-m.npz\"\n",
    "refPath_imgDesc = \"../precomputed/imgGlobalDesc/1-m.npz\"\n",
    "refPath_kpDense = \"../precomputed/kpDesc_C5Tensor/1-m/\"\n",
    "\n",
    "queryPath_imgDesc = \"../precomputed/imgGlobalDesc/5-s.npz\"\n",
    "queryPath_kpDense = \"../precomputed/kpDesc_C5Tensor/5-s/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system parameters\n",
    "\n",
    "topN = 5 # top matches to consider for re-ranking\n",
    "seqL = 1 # number of adjacent frames to consider (setting 4 means using [r-2,r+2] frames centered at index r)\n",
    "depT = 98 # depth range to consider for keypoints\n",
    "MULTITHREADING_ON = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find top-N image matches for query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading global image descriptors for reference and query datasets...\n",
      "Descriptor Data Shapes: (761, 4096) (814, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading global image descriptors for reference and query datasets...\")\n",
    "\n",
    "imgDescRef = np.load(refPath_imgDesc)['arr_0']\n",
    "imgDescQuery = np.load(queryPath_imgDesc)['arr_0']\n",
    "\n",
    "print(\"Descriptor Data Shapes:\",imgDescRef.shape,imgDescQuery.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding global matches using cosine distance...\n",
      "Distance Matrix Shape: (761, 814)\n"
     ]
    }
   ],
   "source": [
    "print(\"Finding global matches using cosine distance...\")\n",
    "\n",
    "distMat = cdist(imgDescRef,imgDescQuery,\"cosine\")\n",
    "topMatches = np.argsort(distMat,axis=0)[:topN,:]\n",
    "\n",
    "numRefImgs, numQueryImgs = distMat.shape[0], distMat.shape[1]\n",
    "\n",
    "print(\"Distance Matrix Shape:\",distMat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Single "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dense conv5 tensors, required for local keypoint and descriptor extraction...\n",
      "Dense Descriptor shape [numImages,numRows*numCols,numFeatureMaps]: (761, 620, 2048)\n",
      "\n",
      "Loading depth masks...\n",
      "Depth masks shape [numImages,numRows,numCols]: (761, 640, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-computed depth masks and c5 tensors (for keypoint and descriptor extraction) \n",
    "\n",
    "def loadC5Tensor(inPath,idx):\n",
    "    return np.load(os.path.join(inPath,\"{0:07d}.npz\".format(idx)))['arr_0']\n",
    "  \n",
    "print(\"Loading dense conv5 tensors, required for local keypoint and descriptor extraction...\")\n",
    "\n",
    "denseFtRefAll = np.array([loadC5Tensor(refPath_kpDense,i1) for i1 in range(numRefImgs)])\n",
    "numFmaps = denseFtRefAll.shape[2]\n",
    "\n",
    "print(\"Dense Descriptor shape [numImages,numRows*numCols,numFeatureMaps]:\",denseFtRefAll.shape)\n",
    "\n",
    "\n",
    "print(\"\\nLoading depth masks...\")\n",
    "\n",
    "depImgRefAll = np.load(refPath_depthData)['arr_0']\n",
    "dImgRows, dImgCols = depImgRefAll.shape[1], depImgRefAll.shape[2]\n",
    "\n",
    "print(\"Depth masks shape [numImages,numRows,numCols]:\",depImgRefAll.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32.         33.03225806]\n"
     ]
    }
   ],
   "source": [
    "c5Shape = np.array([20,31]) # this depends on the spatial resolution of the network used (ResNet101 from RefineNet)\n",
    "\n",
    "# keypoints extracted from c5 are resized using this multiplying factor to obtain their depth value\n",
    "mulFac_dep2c5 = np.array([dImgRows,dImgCols])/c5Shape.astype(float)\n",
    "print(mulFac_dep2c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKP2d(kpInds,resShape_,mulFac_):\n",
    "    kp = np.fliplr(mulFac_*np.array(np.unravel_index(kpInds,resShape_)).transpose()).astype(int)\n",
    "    return kp\n",
    "\n",
    "def getRefSeqInfo(refIdx,maxIndVal,seqLen):\n",
    "    lb = max(refIdx-int(np.floor(seqLen/2.0)),0)\n",
    "    ub = min(refIdx+int(np.ceil(seqLen/2.0)),maxIndVal)\n",
    "    return np.arange(lb,ub)\n",
    "\n",
    "def filterKPs_depth(kp_d,minD,maxD):\n",
    "    return np.argwhere(np.multiply(kp_d<maxD, kp_d>minD))[:,0]\n",
    "\n",
    "def get_cosine_corresponding_vectors(d1,d2):\n",
    "    return 1 - np.sum(d1*d2,axis=1)/(np.linalg.norm(d1,axis=1)*np.linalg.norm(d2,axis=1))\n",
    "\n",
    "def getKpDep(kpIndFlat,depImg,resShape,mulFac):\n",
    "    \n",
    "    kp2d = getKP2d(kpIndFlat,resShape,mulFac)\n",
    "    \n",
    "    kp_dep = depImg[kp2d[:,1].flatten(), kp2d[:,0].flatten()]    \n",
    "    \n",
    "    return kp_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def matchData(denseDescQuery,topMatchIndsList,c5ShapeIn,mulFac,numRefImgs_,numFmaps_,seqLen=seqL,depThresh=depT):\n",
    "    \"\"\"\n",
    "    uses global variables: denseFtRefAll, depImgRefAll\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    kp2IndFlat = np.argmax(denseDescQuery,axis=0)\n",
    "    \n",
    "    for rIdx in topMatchIndsList:\n",
    "        tBegin = time.time()\n",
    "       \n",
    "        seqRange = getRefSeqInfo(rIdx,numRefImgs_,seqLen) \n",
    "                \n",
    "        distMat = np.ones([denseDescQuery.shape[1],len(seqRange)])*100\n",
    "        for lc,k1 in enumerate(seqRange):\n",
    "            \n",
    "            denseDescRef = denseFtRefAll[k1].copy()\n",
    "            \n",
    "            kp1IndFlatRef = np.argmax(denseDescRef,axis=0)\n",
    "            kp1_depth = getKpDep(kp1IndFlatRef,depImgRefAll[k1],c5ShapeIn,mulFac)\n",
    "\n",
    "            inRangeInds = filterKPs_depth(kp1_depth,0,depThresh)\n",
    "\n",
    "            descRefSubset = denseDescRef[kp1IndFlatRef[inRangeInds]]\n",
    "            descQuerySubset = denseDescQuery[kp2IndFlat[inRangeInds]]\n",
    "            \n",
    "            dists = get_cosine_corresponding_vectors(descRefSubset,descQuerySubset)\n",
    "                        \n",
    "            distMat[inRangeInds,lc] = dists.copy()\n",
    "       \n",
    "        minDists = np.min(distMat,axis=1)\n",
    "        validInds1 = np.argwhere(minDists!=100).flatten()\n",
    "        distVals = minDists[validInds1]\n",
    "        \n",
    "        meanDist = np.mean(distVals)            \n",
    "        scores.append(meanDist)        \n",
    "\n",
    "#         print(\"Compute per reference image:\", time.time()-tBegin)\n",
    "        \n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0\n",
      "processed 5\n",
      "processed 10\n",
      "processed 15\n",
      "processed 20\n",
      "processed 25\n",
      "processed 30\n",
      "processed 35\n",
      "processed 40\n",
      "processed 45\n",
      "processed 50\n",
      "processed 55\n",
      "processed 60\n",
      "processed 65\n",
      "processed 70\n",
      "processed 75\n",
      "processed 80\n",
      "processed 85\n",
      "processed 90\n",
      "processed 95\n",
      "processed 100\n",
      "processed 105\n",
      "processed 110\n",
      "processed 115\n",
      "processed 120\n",
      "processed 125\n",
      "processed 130\n",
      "processed 135\n",
      "processed 140\n",
      "processed 145\n",
      "processed 150\n",
      "processed 155\n",
      "processed 160\n",
      "processed 165\n",
      "processed 170\n",
      "processed 175\n",
      "processed 180\n",
      "processed 185\n",
      "processed 190\n",
      "processed 195\n",
      "processed 200\n",
      "processed 205\n",
      "processed 210\n",
      "processed 215\n",
      "processed 220\n",
      "processed 225\n",
      "processed 230\n",
      "processed 235\n",
      "processed 240\n",
      "processed 245\n",
      "processed 250\n",
      "processed 255\n",
      "processed 260\n",
      "processed 265\n",
      "processed 270\n",
      "processed 275\n",
      "processed 280\n",
      "processed 285\n",
      "processed 290\n",
      "processed 295\n",
      "processed 300\n",
      "processed 305\n",
      "processed 310\n",
      "processed 315\n",
      "processed 320\n",
      "processed 325\n",
      "processed 330\n",
      "processed 335\n",
      "processed 340\n",
      "processed 345\n",
      "processed 350\n",
      "processed 355\n",
      "processed 360\n",
      "processed 365\n",
      "processed 370\n",
      "processed 375\n",
      "processed 380\n",
      "processed 385\n",
      "processed 390\n",
      "processed 395\n",
      "processed 400\n",
      "processed 405\n",
      "processed 410\n",
      "processed 415\n",
      "processed 420\n",
      "processed 425\n",
      "processed 430\n",
      "processed 435\n",
      "processed 440\n",
      "processed 445\n",
      "processed 450\n",
      "processed 455\n",
      "processed 460\n",
      "processed 465\n",
      "processed 470\n",
      "processed 475\n",
      "processed 480\n",
      "processed 485\n",
      "processed 490\n",
      "processed 495\n",
      "processed 500\n",
      "processed 505\n",
      "processed 510\n",
      "processed 515\n",
      "processed 520\n",
      "processed 525\n",
      "processed 530\n",
      "processed 535\n",
      "processed 540\n",
      "processed 545\n",
      "processed 550\n",
      "processed 555\n",
      "processed 560\n",
      "processed 565\n",
      "processed 570\n",
      "processed 575\n",
      "processed 580\n",
      "processed 585\n",
      "processed 590\n",
      "processed 595\n",
      "processed 600\n",
      "processed 605\n",
      "processed 610\n",
      "processed 615\n",
      "processed 620\n",
      "processed 625\n",
      "processed 630\n",
      "processed 635\n",
      "processed 640\n",
      "processed 645\n",
      "processed 650\n",
      "processed 655\n",
      "processed 660\n",
      "processed 665\n",
      "processed 670\n",
      "processed 675\n",
      "processed 680\n",
      "processed 685\n",
      "processed 690\n",
      "processed 695\n",
      "processed 700\n",
      "processed 705\n",
      "processed 710\n",
      "processed 715\n",
      "processed 720\n",
      "processed 725\n",
      "processed 730\n",
      "processed 735\n",
      "processed 740\n",
      "processed 745\n",
      "processed 750\n",
      "processed 755\n",
      "processed 760\n",
      "processed 765\n",
      "processed 770\n",
      "processed 775\n",
      "processed 780\n",
      "processed 785\n",
      "processed 790\n",
      "processed 795\n",
      "processed 800\n",
      "processed 805\n",
      "processed 810\n"
     ]
    }
   ],
   "source": [
    "# wrapped in a function for multithreading implemented in the next block\n",
    "def processQueryIndex(qIdx):\n",
    "    topMatchIndsList = topMatches[:,qIdx]\n",
    "\n",
    "    denseFtQuery = loadC5Tensor(queryPath_kpDense,qIdx)\n",
    "    denseDescQuery = denseFtQuery.reshape([-1,numFmaps])\n",
    "    \n",
    "    scores = matchData(denseDescQuery,topMatchIndsList,c5Shape,mulFac_dep2c5,numRefImgs,numFmaps)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "finalScores = []\n",
    "\n",
    "if not MULTITHREADING_ON:\n",
    "\n",
    "    for i in range(numQueryImgs):\n",
    "        s = processQueryIndex(i)    \n",
    "        finalScores.append(s)\n",
    "        if i%5==0:\n",
    "            print(\"processed\",i)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if MULTITHREADING_ON:\n",
    "    from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "    numT = 4\n",
    "    scores = []\n",
    "    for i in range(0,numQueryImgs,numT):\n",
    "        pool = ThreadPool(numT)\n",
    "        s = pool.map(processQueryIndex, range(i,min(i+numT,numQueryImgs)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        scores.append(s)\n",
    "        if i%5==0:\n",
    "            print(\"processed\",i)\n",
    "    finalScores = np.concatenate(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMatch = topMatches[np.argmin(finalScores,axis=1),np.arange(numQueryImgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../out/matchesOut.txt\",np.vstack([np.arange(len(bestMatch)),bestMatch]).transpose(),fmt='%d')\n",
    "np.savetxt(\"../out/matchesBaseline.txt\",np.vstack([np.arange(len(bestMatch)),topMatches[0,:]]).transpose(),fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
